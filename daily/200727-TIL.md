# 200727
# To Day I Learned..
- A. HackerRank Alogrithm (python)
  - 1.Python : Divison
  - 2.Arithmetic Operators
- B. Deeplearning
  - 1.TensorFlow의 기초 사용법
  - 2.예제 dataset(mnist 소개)
  - 3.각 Layer별 역할 개념 및 파라미터 파악(1)
  - 4.각 Layer별 역할 개념 및 파라미터 파악(2)
  - 5.각 Layer별 역할 개념 및 파라미터 파악(3)
  - 6.각 Layer별 역할 개념 및 파라미터 파악(4)

## A. HackerRank Alogrithm (python)
### A-1. Python : Divison
  - #### a와 b가 주어졌을때 a와 b의 몫과 나누기를 출력하는 코드를 작성
  - ```python
    if __name__ == '__main__':
        a = int(input())
        b = int(input())
        print(a // b)
        print(a / b)

### A-2. Arithmetic Operators
  - #### a와 b가 주어졌을때 a + b, a - b, a * b를 출력하는 코드를 작성
  - ```python
    if __name__ == '__main__':
      a = int(input())
      b = int(input())
      print(a + b)
      print(a - b)
      print(a * b)

## B. Deeplearning
### B-1. TensorFlow의 기초 사용법
  - #### tensor를 생성하는 코드, tuple에서 tensor도 가능함
  ```python
  tf.constant([1,2,3])
  tf.constant(((1,2,3),(1,2,3)))
  ```

  - #### numpy의 array를 tensor로 만들수 있음
  ```python
  arr = np.array([1,2,3])
  tf.constant(arr)
  ```

  - #### tensor의 data type 정의는 dtype을 사용
  ```python
  tf.constant([1,2,3], dtype = tf.float32)
  ```

  - #### tf.cast를 사용하여 data type 변경, uint8로 변경
    - python은 자동으로 데이터타입을 잡아주나, 수동으로 변경하여 데이터 처리속도를 빠르게 할수 있음 
  ```python
  tensor = tf.constant([1,2,3], dtype = tf.float32)
  tf.cast(tesnsor, dtype = tf.uint8)
  ```

  - #### numpy array로도 변경가능
  ```python
  tensor = tf.constant([1,2,3], dtype = tf.float32)
  np.array(tensor)
  ```

  - #### 난수 생성
    - Normal Distribution은 중심극한 이론에 의한 연속적인 모양(종 모양)
    - Uniform Distribution은 불연속적이며 일정한 분포를 가진 모양(직사각형 모양)
  
  - #### tensorflow에서의 normal 생성은 shape을 줘야함
  ```python
  tf.random.normal([3,3])
  ```

  - #### tensorflow에서의 Uniform 생성도 shape을 줘야함
  ```python
  tf.random.uniform([4,4])
  ```

### B-2. 예제 dataset(mnist 소개)
  - #### tensorflow에서의 dataset에서 불러오기
  ```python
  from tensorflow.keras import datasets
  mnist = datasets.mnist
  (train_x, train_y),(test_x, test_y) = mnist.load_data()
  train_x.shape
  ```

  - #### Channel을 1로 만들기 
  ```python
  #numpy
  train_x = np.expand_dims(train_x, -1)
  train_x.shape

  # tensorflow
  new_train_x = tf.expand_dims(train_x, -1)
  new_train_x.shape

  # tensorflow 공식홈페이지 가이드
  train_x[..., tf.newaxis]

  # reshape
  reshaped = train_x.reshape([60000, 28, 28 ,1])

  ```
  - #### onehot Encoding
  ```python
  # 원핫 인코딩을 해주는 tensorflow 모듈
  from tensorflow.keras.utils import to_categorical
 
  # 5
  to_categorical(5,10)
  [0,0,0,0,0,1,0,0,0,0]

  # 9
  to_categorical(9,10)
  [0,0,0,0,0,0,0,0,0,1]

  ```
### B-3. 각 Layer별 역할 개념 및 파라미터 파악(1)
  - ####  CNN
    - Feature Extraction (패턴을 찾음)
        - Convolution
        - Pooling
        - Activation function
    - Classification (찾은 패턴으로 예측함)
        - Fully Connected
        - Flatten
        - Dense
        - Dropout
        - Bulid model
  
  - #### Convolution
    - tensorflow의 layers는 keras를 사용함
    - filters: layer에서 나갈 때 몇 개의 filter를 만들 것인지 (a.k.a weights, filters, channels) 필터의 개수만큼 채널의 수가바뀜  
    - kernel_size: filter(Weight)의 사이즈  
    - strides: 몇 개의 pixel을 skip 하면서 훑어지나갈 것인지 (사이즈에도 영향을 줌)  
    - padding: zero padding을 만들 것인지. VALID는 Padding이 없고, SAME은 Padding이 있음 (사이즈에도 영향을 줌)  
    - activation: Activation Function을 만들것인지. 당장 설정 안해도 Layer층을 따로 만들 수 있음
  ```python
  tf.keras.layers.Conv2D(filters=3, kernel_size=(3,3), strides=(1,1), activation='relu')
  ```
  
  - #### 조금 더 간단한 코드
  ```python
  tf.keras.layers.Conv2D(3, 3, 1, 'SAME')
  ```

  - #### layer를 통과해보기
  ```python
  # image 생성 - int형이면 오류나기에 float형으로 바꿈
  image = tf.cast(image, dtype = tf.float32)
  image.dtype

  # layer 생성
  layer = tf.keras.layers.Conv2D(3, 3, 1, 'SAME')
  layer

  # layer에서 필터의 값을 다르게 주면 아래의 channel에도 갯수가 다르게 나옴
  # same이 아니라 valid를 주면 이미지 사이즈도 달라짐 (28,28이 아니게됨)
  output = layer(image)
  output
  ```

### B-4. 각 Layer별 역할 개념 및 파라미터 파악(2)
- #### weight 불러오기
  - weight는 리스트로 받아오며 앞에는 weight, 뒤는 bias임
```python
weight = layer.get_weights()
weight

# weight는 리스트로 받아짐
len(weight)

# weight의 1번째는 weight고 2번째는 bios임
weight[0].shape, weight[1].shape
```

- #### 시각화 해서보기
```python
# flatten histogram, 원본 이미지, 필터링된 이미지 순으로 나옴

plt.figure(figsize=(24, 8))
plt.subplot(131)
plt.hist(output.numpy().ravel(), range=[-2, 2]) # output.numpy().ravel은 flatten하게 해줌
plt.ylim(0, 500)

plt.subplot(132)
plt.title(weight[0].shape)
plt.imshow(weight[0][:, :, 0, 0], 'gray')

plt.subplot(133)
plt.title(output.shape)
plt.imshow(output[0, :, :, 0], 'gray')
plt.grid(False)
plt.colorbar()
plt.show()
```

- #### Activation Function
  - relu : 0 미만인 숫자는 모두 0으로 바꿈
```python
# output에 ReLU 함수 적용
act_layer = tf.keras.layers.ReLU()
act_output = act_layer(output)

# shape는 그대로임
output.shape

# 0미만은 0으로 바뀜
np.min(act_output), np.max(act_output)

# 시각화
plt.figure(figsize=(24, 8))
plt.subplot(121)
plt.hist(act_output.numpy().ravel(), range=[-2, 2])
plt.ylim(0, 500)

plt.subplot(122)
plt.title(act_output.shape)
plt.imshow(act_output[0, :, :, 0], 'gray')
plt.grid(False)
plt.show()
```

- #### Pooling
  - Pooling :이미지 압축시키는것, 따라서 이미지 사이즈가 원본보다 작아짐
  - maxpooling : 이미지 압축시 가장 큰 숫자만 남기는 것
```python
# maxpooling은 가장 큰 값만 가져옴, 2,2 는 2 by 2 사이즈
pool_layer = tf.keras.layers.MaxPool2D(pool_size = (2,2), strides=(2,2), padding='SAME')
pool_output = pool_layer(act_output)

# 28 by 28 size가 14 by 14로 작아짐
pool_output.shape

# 시각화
plt.figure(figsize=(24,8))
plt.subplot(121)
plt.hist(pool_output.numpy().ravel(), range = [-2,2])
plt.ylim(0,100)

plt.subplot(122)
plt.title(pool_output.shape)
plt.imshow(pool_output[0,:,:,0], 'gray')
plt.grid(False)
plt.colorbar()
plt.show()
```

### B-5.각 Layer별 역할 개념 및 파라미터 파악(3)
  - #### Fully Connected
    - inputlayer은 flatten 해야함
    - inputlayer 와 outputlayer 사이에 hiddenlayer가 있음
    - 각각의 layer는 weight로 연결이 되고, bias가 추가됨
    - outputlayer는 마지막에 sigmoid나 softmax 함수로 activation함
  - #### Flatten
    - 평평하게 피는것
  ```python
  layer = tf.keras.layers.Flatten()
  flatten = layer(output)
  flatten.shape, output.shape, 1 * 28 * 28 * 3

  # 시각화
  plt.figure(figsize = (24,8))
  plt.subplot(211)
  plt.hist(flatten.numpy().ravel()) # batchsize는 남아있으니, ravel을 해줘야함

  plt.subplot(212)
  plt.imshow(flatten[:,:100], 'jet')
  plt.show()
  ```

  - #### Dense
    - 하나씩 모두 연결하는것
  ```python
  # 1 * 28 * 28 * 3 = 2352를 32개의 node로 받는다
  layer = tf.keras.layers.Dense(32, activation='relu')
  output = layer(flatten)
  output.shape # output의 shape도 1,32로 됨

  # layer를 하나더 만들어서 한번더 통과시킴
  layer_2 = layer = tf.keras.layers.Dense(10, activation='relu')
  output_example = layer_2(output)
  output_example.shape # output_example은 10이 됨  
  ```

### B-6.각 Layer별 역할 개념 및 파라미터 파악(4)
  - #### DropOut
    - overfitting을 없애기 위해 노드 몇개를 꺼버리는것,
    - 학습시에만 가능함
    - 테스트시에는 꺼졋던 dropout이 다시 켜짐
  ```python
  layer = tf.keras.layers.Dropout(0.7) # 비율을 값으로 줌
  output = layer(output)
  output.shape # shape가 달라지지는 않음
  ```

  - #### Build Model
    - 앞에서 했던것들을 한번에 연결하여 실제 모델을 생성하기
  ```python
  from tensorflow.keras import layers

  # 시작하는 데이터의 모양
  input_shape = (28,28,1)

  # class의 갯수를 지정
  num_classes = 10

  # 위에서 layers를 미리 불러왔기에 따로 앞에걸 안적고 비로 layers로 사용가능
  inputs = layers.Input(shape=input_shape)

  # Feature Extraction
  net = layers.Conv2D(32, 3, padding='SAME')(inputs)
  net = layers.Activation('relu')(net)
  net = layers.Conv2D(32, 3, padding='SAME')(net)
  net = layers.Activation('relu')(net)
  net = layers.MaxPool2D((2, 2))(net)
  net = layers.Dropout(0.25)(net)

  net = layers.Conv2D(64, 3, padding='SAME')(net)
  net = layers.Activation('relu')(net)
  net = layers.Conv2D(64, 3, padding='SAME')(net)
  net = layers.Activation('relu')(net)
  net = layers.MaxPool2D((2, 2))(net)
  net = layers.Dropout(0.25)(net)

  # Fully Connected
  net = layers.Flatten()(net)
  net = layers.Dense(512)(net)
  net = layers.Activation('relu')(net)
  net = layers.Dropout(0.25)(net)
  net = layers.Dense(10)(net)  # class 갯수와 동일해야함(정답 종류의 갯수)
  net = layers.Activation('softmax')(net)  # softmax를 각 class별로 확률을 볼수있게 됨
  model = tf.keras.Model(inputs=inputs, outputs=net, name='Basic_CNN')
  ```

- #### summary
  - tensorflow는 model을 만들면 summary를 볼수 있음
```python
model.summary()
```