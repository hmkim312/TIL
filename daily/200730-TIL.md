# 200730
# To Day I Learned..
- A. HackerRank Alogrithm (python)
  - 1.Find the Runner-Up Score!
  - 2.list
  - 3.tuple
  - 4.Compare the Triplets
  - 5.Birthday Cake Candles
  
- B. Deeplearning
  - 1.2-11. PyTorch에서 데이터 불러오기
  - 2.2-12~13. 각 Layer별 역할 개념 및 파라미터 파악(1)
  - 3.2-12~13. 각 Layer별 역할 개념 및 파라미터 파악(2)
  - 4.2-14. PyTorch : Optimization _ Training
  - 5.2-15. PyTorch : Evaluating _ Predicting

## A. HackerRank Alogrithm (python)
### A-1. Find the Runner-Up Score!
  - #### 주어진 배열에서 준우승을 출력하는것
  - #### 배열을 set으로 중복제거 후 sort로 정렬하여 맨뒤에 하나를(최고점수) pop으로 제거 후 마지막것을 출력  
  ```python
  if __name__ == '__main__':
    n = int(input())
    arr = map(int, input().split())
    arr = list(arr)
    ls= list(set(arr))
    ls.sort()
    ls.pop()
    print(ls[-1])
  ```
### A-2.list
  - #### 빈 list를 만들고, 입력되는 리스트의 함수에 맞게 작동을 시키는것
  - #### if문을 활용하여 코드를 작성하였다.
  ```python
  if __name__ == '__main__':
    N = int(input())
    arr = []
    for i in range(N):
        k = input().split()
        if k[0] == 'insert':
            arr.insert(int(k[1]), int(k[2]))

        elif k[0] == 'print':
            print(arr)

        elif k[0] == 'remove':
            arr.remove(int(k[1]))

        elif k[0] == 'append':
            arr.append(int(k[1]))

        elif k[0] == 'sort':
            arr.sort()

        elif k[0] == 'reverse':
            arr.reverse()

        elif k[0] == 'pop':
            arr.pop()
  ```
### A-3.tuple
  - #### 주어진 숫자를 tuple로 만들고, hash값을 출력하는것
  - #### hash()는 객체의 hash 값을 반환함.
  ```python
  if __name__ == '__main__':
    n = int(input())
    integer_list = map(int, input().split())
    print(hash(tuple(integer_list)))
  ```

### A-4.Compare the Triplets
  - #### a,b의 배열의 원소의 크기를 확인후, 더 큰쪽에 점수를 +1씩 하는 함수를 작성
  - #### 배열의 index를 for문으로 돌려서 if문으로 더 큰쪽에 +1를 하는 코드를 작성 후 해당 점수들을 return하는 함수를 작성하였다.
  ```python
  def compareTriplets(a, b):
    alice = 0
    bob = 0
    for i in range(len(a)):
        if a[i] > b[i]:
            alice += 1
        elif a[i] < b[i]:
            bob += 1
    return alice, bob
  ```
### A-5.Birthday Cake Candles
  - #### 조카의 나이가 주어지고, 그 나이만큼의 초가 있으며, 조카는 가장 긴 초의 촛불만 끌수 있다.
  - #### 너무 어렵게 생각했음, ar_count는 그냥 초의 갯수인데, 그게 아니라 그거보다 더 큰수가 올수도 있을거라고 생각했음, 문제를 잘 읽어야할듯..
  - #### 어쨋든 배열에서 가장 큰 수를 찾고, 가장 큰 수의 count를 확인하는 birthdayCakeCandles 함수를 작성
  ```python
  #!/bin/python3

  import math
  import os
  import random
  import re
  import sys

  # Complete the birthdayCakeCandles function below.
  def birthdayCakeCandles(ar):
      max_candle = max(ar)
      return ar.count(max_candle)
              
  if __name__ == '__main__':
      fptr = open(os.environ['OUTPUT_PATH'], 'w')

      ar_count = int(input())

      ar = list(map(int, input().rstrip().split()))

      result = birthdayCakeCandles(ar)

      fptr.write(str(result) + '\n')

      fptr.close()
  ```

## B. Deeplearning
### B-1.1.2-11. PyTorch에서 데이터 불러오기
  - #### pytorch에서 데이터를 불러오는 방법에 대해 학습 및 시각화를 해봄
  - #### Data Loader 부르기
    - pytorch는 dataloader를 불러 model에 넣음
    ```python
    # batch size를 data loader에 직접 넣어줌
    batch_size = 32
    test_batch_size = 32

    # train_loader 생성
    train_loader = torch.utils.data.DataLoader(
    datasets.MNIST('dataset/', train = True, download = True,
                  transform = transforms.Compose([
                      transforms.ToTensor(),
                      transforms.Normalize(mean=(0.5,), std = (0.5,))
                  ])),
    batch_size=batch_size,
    shuffle=True)

    # test_loader 생성
    test_loader = torch.utils.data.DataLoader(
    datasets.MNIST('dataset', train = False,
                  transform = transforms.Compose([
                      transforms.ToTensor(),
                      transforms.Normalize((0.5,), (0.5))
                  ])),
    batch_size = test_batch_size,
    shuffle = True)
    ```
  - #### 첫번째 iteratrion에서 나오는 데이터를 확인하기
    - PyTorch는 TensorFlow와 다르게 [Batch Size, Channel, Height, Width] 임을 명시해야함
    ```python
    images, labels = next(iter(train_loader))

    images.shape
    # torch.Size([32, 1, 28, 28])

    labels.shape
    # torch.Size([32])
    ```

  - #### 데이터 시각화
    - 뽑은 데이터를 시각화 해보기
    ```python
    import numpy as np
    import matplotlib.pyplot as plt
    %matplotlib inline

    torch_image = torch.squeeze(images[0])
    torch_image.shape

    # 이미지를 numpy arr로 바꿈
    image = torch_image.numpy()
    image.shape

    # label 생성
    label = labels[0].numpy()
    label.shape

    plt.title(label)
    plt.imshow(image, 'gray')
    plt.grid(False)
    plt.show()
    ```
### B-2.2-12~13. 각 Layer별 역할 개념 및 파라미터 파악(1)
  - #### Pytorch의 later 이해하기
    - 이전 실습 코드불러오기
    ```python
    import torch
    from torchvision import datasets, transforms

    import numpy as np
    import matplotlib.pyplot as plt

    %matplotlib inline

    train_loader = torch.utils.data.DataLoader(
    datasets.MNIST('dataset', train=True, download=True,
                   transform=transforms.Compose([
                       transforms.ToTensor()
                   ])),
    batch_size=1)

    image, label = next(iter(train_loader))

    # 시각화
    plt.title(label[0])
    plt.imshow(image[0,0,:,:],'gray')
    plt.grid(False)
    plt.show()
    ```

  - #### 각 Layer 별 설명
    - Network 쌓기 위한 준비
    ```python
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    ```

  - #### Convolution
    - in_channels: 받게 될 channel의 갯수
    - out_channels: 보내고 싶은 channel의 갯수  
    - kernel_size: 만들고 싶은 kernel(weights)의 사이즈
    ```python
    layer = nn.Conv2d(in_channels=1, out_channels=20, kernel_size=5, stride=1)

    # 이런 방법도 있음
    layer = nn.Conv2d(1,20,5,1).to(torch.device('cpu'))
    ```

    - weight 시각화를 위해 slice하고 numpy화
    ```python
    weight = layer.weight
    weight.shape
    # torch.Size([20, 1, 5, 5])
    ```

    - 여기서 weight는 학습 가능한 상태이기 때문에 바로 numpy로 뽑아낼 수 없음
    - detach() method는 그래프에서 잠깐 빼서 gradient에 영향을 받지 않게 함
    ```python
    weight = weight.detach().numpy()
    weight.shape
    # (20, 1, 5, 5)

    # 시각화
    plt.imshow(weight[0,0,:,:], 'jet')
    plt.colorbar()
    plt.show()
    ```

    - output 시각화 준비를 위해 numpy화
    ```python
    output_data = layer(image)
    output_data = output_data.data
    output = output_data.cpu().numpy()
    output.shape
    #(1, 20, 24, 24)
    ```

    - input으로 들어간 이미지 numpy화
    ```python
    image_arr = image.numpy()
    image_arr.shape
    ```

    - Input, Weight, Output 시각화
    ```python
    plt.figure(figsize=(15,30))
    plt.subplot(131)
    plt.title('Input')
    plt.grid(False)
    plt.imshow(np.squeeze(image_arr), 'gray')

    plt.subplot(132)
    plt.title('Weight')
    plt.grid(False)
    plt.imshow(weight[0,0,:,:], 'jet')

    plt.subplot(133)
    plt.title('Output')
    plt.grid(False)
    plt.imshow(output[0,0,:,:], 'gray')
    plt.show()
    ```
  
  - #### Pooling
    - input을 먼저 앞에 넣고, 뒤에 kernel 사이즈와 stride를 순서대로 넣음
    ```python
    image.shape
    # torch.Size([1, 1, 28, 28])

    pool = F.max_pool2d(image, 2,2)
    pool.shape
    # torch.Size([1, 1, 14, 14])
    ```

    - MaxPool Layer는 weight가 없기 때문에 바로 numpy()가 가능
    ```python
    pool_arr = pool.numpy()
    pool_arr.shape
    # (1, 1, 14, 14)

    image.shape
    # torch.Size([1, 1, 28, 28])
    ```

    - 원본과 맥스풀링된 이미지 시각화하여 비교
    ```python
    plt.figure(figsize=(10, 15))
    plt.subplot(121)
    plt.title('Input')
    plt.grid(False)
    plt.imshow(np.squeeze(image_arr), 'gray')

    plt.subplot(122)
    plt.title('Output')
    plt.grid(False)
    plt.imshow(np.squeeze(pool_arr), 'gray')
    plt.show()
    ```
  - #### Linear
    - nn.Linear는 2d가 아닌 1d만 들어가기 때문에 .view() 1D로 펼쳐줘야함
    ```python
    # 펼쳐주는것
    flatten = image.view(1, 28 * 28)
    flatten.shape
    # torch.Size([1, 784])

    lin = nn.Linear(784, 10)(flatten)
    lin.shape
    # torch.Size([1, 10])

    lin
    # tensor([[ 0.2022, -0.0297, -0.2828, -0.1667, -0.3458, -0.1822, -0.4433,  0.5148, -0.3949, -0.1795]], grad_fn=<AddmmBackward>)
    ```

    - 시각화
    ```python
    plt.imshow(lin.detach().numpy(),'jet')
    plt.grid(False)
    plt.show()
    ```

  - #### Sotfmax
    - 결과를 numpy로 꺼내기 위해선 weight가 담긴 Linear에 weight를 꺼줘야함
    ```python
    with torch.no_grad():
      flatten = image.view(1, 28 * 28)
      lin = nn.Linear(784, 10)(flatten)
      softmax = F.softmax(lin, dim = 1)

    softmax
    # tensor([[0.0925, 0.1005, 0.0851, 0.1183, 0.0955, 0.1172, 0.0778, 0.1051, 0.1097, 0.0984]])

    # 총합 확인(sotfmax는 1이여야함)
    np.sum(softmax.numpy())
    ```

### B-3.2-12~13. 각 Layer별 역할 개념 및 파라미터 파악(2)
  - #### Layer 쌓기
    - nn 과 nn.functional의 차이점
      - nn은 학습 파라미터가 담긴 것 
      - nn.functional은 학습 파라미터가 없는 것이라 생각하면 간단
    - Layer는 class를 활용하여 쌓는것이 편함
    ```python
    # class를 활용
    class Net(nn.Module):
        def __init__(self):
            super(Net, self).__init__()
            self.conv1 = nn.Conv2d(1, 20, 5, 1)
            self.conv2 = nn.Conv2d(20, 50, 5, 1)
            # flatten
            self.fc1 = nn.Linear(4 * 4 * 50, 500)
            self.fc2 = nn.Linear(500, 10)

        def forward(self, x):
            # Feature Extraction
            x = F.relu(self.conv1(x))
            x = F.max_pool2d(x, 2, 2)
            x = F.relu(self.conv2(x))
            x = F.max_pool2d(x, 2, 2)
            
            # Fully Connected (classficiation)
            # flatten
            x = x.view(-1, 4 * 4 * 50)
            x = F.relu(self.fc1(x))
            x = self.fc2(x)
            return F.log_softmax(x, dim = 1)
    ```

    - image를 model에 넣어서 결과 확인
    ```python
    # 모델 정의
    model = Net()

    # 모델에 image 넣기
    result = model.forward(image)
    result
    #tensor([[-2.2743, -2.2449, -2.2842, -2.2880, -2.3116, -2.2911, -2.3086, -2.3169,-2.2942, -2.4213]], grad_fn=<LogSoftmaxBackward>)
    ```
### B-4.2-14. pyTorch : Optimization _ Training
  - #### Optimization
    - Model과 Optimization 설정
    ```python
    # model 설정
    model = Net().to(device)

    # optimizer 설정
    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.5)
    ```
    - Parameters 확인
      - Weight, bais를 순서대로 보여줌
    ```python
    params = list(model.parameters())
    for i in range(8):
        print(params[i].size())
    ```
  - #### Before Training
    -  학습하기 전에 Model이 Train할 수 있도록 Train Mode로 변환
    - Convolution 또는 Linear 뿐만 아니라, DropOut과 추후에 배우게 될 Batch Normalization과 같이 parameter를 가진 Layer들도 학습하기 위해 준비
    ```python
    # trian mode로 변환
    model.train() 

    # 모델에 넣기 위한 첫 Batch 데이터 추출
    data, target = next(iter(train_loader))

    # 추출한 Batch 데이터를 cpu 또는 gpu와 같은 device에 compile
    use_cuda = not no_cuda and torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")
    data, target = data.to(device), target.to(device)
    data.shape, target.shape
    # (torch.Size([64, 1, 28, 28]), torch.Size([64]))

    # gradients를 clear해서 새로운 최적화 값을 찾기 위해 준비 (optimizer 를 clear)
    optimizer.zero_grad()

    # 준비한 데이터를 model에 input으로 넣어 output을 얻음
    output = model(data)

    # Model에서 예측한 결과를 Loss Function에 넣음
    # 여기 예제에서는 Negative Log-Likelihood Loss 라는 Loss Function을 사용
    loss = F.nll_loss(output, target)

    # Back Propagation을 통해 Gradients를 계산(기울기 계산)
    loss.backward()

    # 계산된 Gradients는 계산된 걸로 끝이 아니라 Parameter에 Update
    optimizer.step()
    ```

  - #### Start Training
    - 위의 최적화 과정을 반복하여 학습 시작
    - for문을 이용하여 한번에 하기
    - 점점 loss가 떨어지는 것을 볼수 있음
    ```python
    epochs = 1
    # 로그 확인하기 위한 범위
    log_interval = 100

    for epoch in range(1, epochs + 1):
    # train mode
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        
        # 로그 확인
        if batch_idx % log_interval == 0:
            print('Train Epoch : {} [{}/{} ({:.0f}%)]\tLoss: {:6f}'.format(
            epoch, batch_idx * len(data), len(train_loader.dataset),
            100 * batch_idx / len(train_loader), loss.item()
            ))

    # Train Epoch : 1 [0/60000 (0%)]	Loss: 1.957346
    # Train Epoch : 1 [6400/60000 (11%)]	Loss: 1.508198
    # Train Epoch : 1 [12800/60000 (21%)]	Loss: 1.242586
    # Train Epoch : 1 [19200/60000 (32%)]	Loss: 0.877059
    # Train Epoch : 1 [25600/60000 (43%)]	Loss: 0.776707
    # Train Epoch : 1 [32000/60000 (53%)]	Loss: 0.529819
    # Train Epoch : 1 [38400/60000 (64%)]	Loss: 0.475585
    # Train Epoch : 1 [44800/60000 (75%)]	Loss: 0.449998
    # Train Epoch : 1 [51200/60000 (85%)]	Loss: 0.412482
    # Train Epoch : 1 [57600/60000 (96%)]	Loss: 0.543922
    ```

### 5.2-15. PyTorch : Evaluating _ Predicting
  - #### Evaluation (평가)
    - 앞에서 model.train() 모드로 변한 것처럼 평가 할 때는 model.eval()로 설정
    - Batch Normalization이나 Drop Out 같은 Layer들을 잠금
    ```python
    # 평가 모드
    model.eval()

    test_loss = 0
    correct = 0

    # 메모리 사용량을 줄이고, 속도를 높이기 위해 grad를 꺼줌
    with torch.no_grad():
        data, target = next(iter(test_loader))
        data, target = data.to(device), target.to(device)
        output = model(data)
        
        test_loss += F.nll_loss(output, target, reduction='sum').item()
        
        # 예측한 결과 보기
        pred = output.argmax(dim = 1, keepdim = True)
        correct = pred.eq(target.view_as(pred)).sum().item()

    # test loss 확인
    test_loss

    # 맞춘게 몇%인지 확인, 전체가 64개여서 64로 나눔
    # pred.eq(target.view_as(pred)).sum().item()는 맞춘 것은 True라서 1로 나옴
    correct / 64

    test_loss /= len(test_loader.dataset)
    test_loss
    ```

  - #### 전체 데이터에 적용
    - for문을 이용하여 전체 test 데이터에 적용
    ```python
    model.eval()

    test_loss = 0
    correct = 0

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += F.nll_loss(output, target, reduction= 'sum').item()
            pred = output.argmax(dim = 1, keepdim =True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            
    test_loss /= len(test_loader.dataset)

    print('\nTest set: Average Loss : {:4f}, Accuracy : {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset), 100 * correct / len(test_loader.dataset)))

    # Test set: Average Loss : 0.482159, Accuracy : 8650/10000 (86%)
    ```
  - #### 앞에서 배운 Train과 Test를 한번에 진행하기
    - 1epoch당 test 결과가 도출되게
    ```python
    # epochs 설정
    epochs = 3
    no_cuda = False
    log_interval = 100

    for epoch in range(1, epochs + 1):
      # Train Mode
      model.train()

      for batch_idx, (data, target) in enumerate(train_loader):
          data, target = data.to(device), target.to(device)
          optimizer.zero_grad()  # backpropagation 계산하기 전에 0으로 기울기 계산
          output = model(data)
          loss = F.nll_loss(output, target)  # https://pytorch.org/docs/stable/nn.html#nll-loss
          loss.backward()  # 계산한 기울기를 
          optimizer.step()  

          if batch_idx % log_interval == 0:
              print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                  epoch, batch_idx * len(data), len(train_loader.dataset),
                  100. * batch_idx / len(train_loader), loss.item()))
    
      # 평가 모드
      model.eval()

      test_loss = 0
      correct = 0

      with torch.no_grad():
          for data, target in test_loader:
              data, target = data.to(device), target.to(device)
              output = model(data)
              test_loss += F.nll_loss(output, target, reduction= 'sum').item()
              pred = output.argmax(dim = 1, keepdim =True)
              correct += pred.eq(target.view_as(pred)).sum().item()

      test_loss /= len(test_loader.dataset)

      print('\nTest set: Average Loss : {:4f}, Accuracy : {}/{} ({:.0f}%)\n'.format(
          test_loss, correct, len(test_loader.dataset), 100 * correct / len(test_loader.dataset)))
    ```
    - 위의 코드 실행 결과 
      - 점점 loss는 줄고 accuracy는 증가하는 것을 알수 있음
    ```
      Train Epoch: 1 [0/60000 (0%)]	Loss: 0.164533
      Train Epoch: 1 [6400/60000 (11%)]	Loss: 0.390823
      Train Epoch: 1 [12800/60000 (21%)]	Loss: 0.405805
      Train Epoch: 1 [19200/60000 (32%)]	Loss: 0.172504
      Train Epoch: 1 [25600/60000 (43%)]	Loss: 0.275285
      Train Epoch: 1 [32000/60000 (53%)]	Loss: 0.399491
      Train Epoch: 1 [38400/60000 (64%)]	Loss: 0.233075
      Train Epoch: 1 [44800/60000 (75%)]	Loss: 0.503213
      Train Epoch: 1 [51200/60000 (85%)]	Loss: 0.242056
      Train Epoch: 1 [57600/60000 (96%)]	Loss: 0.284364
      
      Test set: Average Loss : 0.228095, Accuracy : 9344/10000 (93%)

      Train Epoch: 2 [0/60000 (0%)]	Loss: 0.180398
      Train Epoch: 2 [6400/60000 (11%)]	Loss: 0.168027
      Train Epoch: 2 [12800/60000 (21%)]	Loss: 0.153454
      Train Epoch: 2 [19200/60000 (32%)]	Loss: 0.184043
      Train Epoch: 2 [25600/60000 (43%)]	Loss: 0.201183
      Train Epoch: 2 [32000/60000 (53%)]	Loss: 0.301697
      Train Epoch: 2 [38400/60000 (64%)]	Loss: 0.218239
      Train Epoch: 2 [44800/60000 (75%)]	Loss: 0.174217
      Train Epoch: 2 [51200/60000 (85%)]	Loss: 0.243847
      Train Epoch: 2 [57600/60000 (96%)]	Loss: 0.202507

      Test set: Average Loss : 0.179864, Accuracy : 9487/10000 (95%)

      Train Epoch: 3 [0/60000 (0%)]	Loss: 0.270935
      Train Epoch: 3 [6400/60000 (11%)]	Loss: 0.205819
      Train Epoch: 3 [12800/60000 (21%)]	Loss: 0.123366
      Train Epoch: 3 [19200/60000 (32%)]	Loss: 0.121298
      Train Epoch: 3 [25600/60000 (43%)]	Loss: 0.170279
      Train Epoch: 3 [32000/60000 (53%)]	Loss: 0.189426
      Train Epoch: 3 [38400/60000 (64%)]	Loss: 0.141288
      Train Epoch: 3 [44800/60000 (75%)]	Loss: 0.062443
      Train Epoch: 3 [51200/60000 (85%)]	Loss: 0.227330
      Train Epoch: 3 [57600/60000 (96%)]	Loss: 0.095481

      Test set: Average Loss : 0.154556, Accuracy : 9564/10000 (96%)
      ```