# 200801
# To Day I Learned..
- A. HackerRank Alogrithm (python)
  - 1.Mutations
  - 2.Find a string

- B. Deeplearning
  - 1.2-16. TensorFlow2.0과 PyTorch 비교

## A. HackerRank Alogrithm (python)
### A-1. Mutations
  - #### string을 list로 바꾸고 주어진 i번쨰에 있는 원소를 character로 바꾸는 함수를 작성
  ```python
  def mutate_string(string, position, character):
      ls = list(string)
      ls[position] = character
      new_stirng = ''.join(ls)
      return new_stirng

  if __name__ == '__main__':
      s = input()
      i, c = input().split()
      s_new = mutate_string(s, int(i), c)
      print(s_new)
  ```

### A-2. Find a string
  - #### 첫번째 문자가 주어지고, 두번째 문자가 주어졌을때 첫번쨰 문자가 두번째 문자로 시작하는 횟수를 반환
  - #### python의 startswith는 문자열이 특정문자로 시작하는지 여부를 알려줌, 해당 함수를 이용하여 for문으로 코드 작성

  ```python
  # startswith는 문자열이 특정문자로 시작하는지 여부를 알려준다

  def count_substring(string, sub_string):
      count = 0
      for i in range(0, len(string)):
          if string[i:].startswith(sub_string):
              count += 1
      return count

  if __name__ == '__main__':
      string = input().strip()
      sub_string = input().strip()
      
      count = count_substring(string, sub_string)
      print(count)
  ```

## B. Deeplearning
### B-1.2-16. TensorFlow2.0과 PyTorch 비교
  - #### TensorFlow 2.0
    - TensorFlow가 2.0 이상이 되면서 keras를 포함시켜 코드적으로 많이 깔끔해졌음
    - 데이터 로드 및 전처리 ->  모델 생성 -> 하이퍼 파라미터 튜닝 -> 모델 compile -> 학습 -> 예측 순으로 진행
    - 모델 컴파일시에 하이퍼 파리미터 튜닝의 옵션들을 넣어주기가 편함

    ```python
    # tensorflow import
    import tensorflow as tf
    from tensorflow.keras import layers
    from tensorflow.keras import datasets 

    # Hyperparameter Tunning
    # notebook 맨 위에 두고 입맛에 맞게 조절
    num_epochs = 1
    batch_size = 64

    learning_rate = 0.001

    dropout_rate = 0.7

    input_shape = (28, 28, 1)
    num_classes = 10


    # Preprocess
    # 데이터 load
    (train_x, train_y), (test_x, test_y) = datasets.mnist.load_data()

    # data 차원 생성(맞춰서)
    train_x = train_x[..., tf.newaxis]
    test_x = test_x[..., tf.newaxis]

    # 데이터 rescale
    train_x = train_x / 255.
    test_x = test_x / 255.

    # 모델 설계
    inputs = layers.Input(input_shape)
    net = layers.Conv2D(32, (3, 3), padding='SAME')(inputs)
    net = layers.Activation('relu')(net)
    net = layers.Conv2D(32, (3, 3), padding='SAME')(net)
    net = layers.Activation('relu')(net)
    net = layers.MaxPooling2D(pool_size=(2, 2))(net)
    net = layers.Dropout(dropout_rate)(net)

    net = layers.Conv2D(64, (3, 3), padding='SAME')(net)
    net = layers.Activation('relu')(net)
    net = layers.Conv2D(64, (3, 3), padding='SAME')(net)
    net = layers.Activation('relu')(net)
    net = layers.MaxPooling2D(pool_size=(2, 2))(net)
    net = layers.Dropout(dropout_rate)(net)

    net = layers.Flatten()(net)
    net = layers.Dense(512)(net)
    net = layers.Activation('relu')(net)
    net = layers.Dropout(dropout_rate)(net)
    net = layers.Dense(num_classes)(net)
    net = layers.Activation('softmax')(net)

    model = tf.keras.Model(inputs=inputs, outputs=net, name='Basic_CNN')

    # Model is the full model w/o custom layers
    # 모델 compile
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),  # Optimization
                  loss='sparse_categorical_crossentropy',  # Loss Function 
                  metrics=['accuracy'])  # Metrics / Accuracy

    # Training
    model.fit(train_x, train_y,   
                batch_size=batch_size, 
                shuffle=True)

    model.evaluate(test_x, test_y, batch_size=batch_size)
    ```

  - #### Pytorch
    - 모델을 class로 만드는것이 인상 깊음
    - 데이터 로더를 만들어서 데이터 로딩을 해야하는것(텐서보다 복잡함)
    - 모델 컴파일시에 파라미터를 넣는것이 텐서보다 적음
    - 다만 모델 train할때 파라미터들을 넣고, train mode와 test mode를 바꿔줘야함
    - 프로그래스bar를 따로 제공하여 주지 않아. print 문을 이용하여 작성해야함.
    - tensor와 같이 쓰는것을 장려하나, 코드의 복잡도가 있어서 tensor보단 좀 덜 손이 간다.
    ```python
    # pytorch import
    import torch

    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim

    from torchvision import datasets, transforms

    # 하이퍼 파라미터 튜닝
    seed = 1

    lr = 0.001
    momentum = 0.5

    batch_size = 64
    test_batch_size = 64

    epochs = 5

    no_cuda = False
    log_interval = 100

    # model 생성

    class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5, 1)
        self.conv2 = nn.Conv2d(20, 50, 5, 1)
        self.fc1 = nn.Linear(4*4*50, 500)
        self.fc2 = nn.Linear(500, 10)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, 2, 2)
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, 2, 2)
        x = x.view(-1, 4*4*50)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)

    # data loader 생성
    torch.manual_seed(seed)

    use_cuda = not no_cuda and torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")

    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}


    train_loader = torch.utils.data.DataLoader(
        datasets.MNIST('../data', train=True, download=True,
                      transform=transforms.Compose([
                          transforms.ToTensor(),
                          transforms.Normalize((0.1307,), (0.3081,))
                      ])),
        batch_size=batch_size, shuffle=True, **kwargs)



    test_loader = torch.utils.data.DataLoader(
        datasets.MNIST('../data', train=False, transform=transforms.Compose([
                          transforms.ToTensor(),
                          transforms.Normalize((0.1307,), (0.3081,))
                      ])),
        batch_size=test_batch_size, shuffle=True, **kwargs)

    # optimization
    model = Net().to(device)
    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)

    # Training
    for epoch in range(1, epochs + 1):
    # Train Mode
    model.train()

    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()  # backpropagation 계산하기 전에 0으로 기울기 계산
        output = model(data)
        loss = F.nll_loss(output, target)  # https://pytorch.org/docs/stable/nn.html#nll-loss
        loss.backward()  # 계산한 기울기를 
        optimizer.step()  

        if batch_idx % log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.item()))
    
    # Test mode
    model.eval()  # batch norm이나 dropout 등을 train mode 변환
    test_loss = 0
    correct = 0
    with torch.no_grad():  # autograd engine, 즉 backpropagatin이나 gradient 계산 등을 꺼서 memory usage를 줄이고 속도를 높임
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss
            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability
            correct += pred.eq(target.view_as(pred)).sum().item()  # pred와 target과 같은지 확인

    test_loss /= len(test_loader.dataset)

    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))
    ```
